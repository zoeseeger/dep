from datetime import datetime
from datetime import timedelta

from airflow import DAG
from airflow.hooks.postgres_hook import PostgresHook
from airflow.models import Variable
from airflow.operators.python_operator import PythonOperator
from pendulum import timezone

glinda_conn_id = "glinda_etl_airflow"

p_dag_id = "{{dag_id}}"
p_description = "Upsert into {{schema_table}}."
p_owner = "{{user}}"
p_timezone = timezone("Australia/Melbourne")
p_pool = "{{pool}}"
env = Variable.get("environment")

if env == "prod":
    v_start_date = datetime({{year}}, {{month}}, {{day}}, 0, 0, tzinfo=p_timezone)
    v_schedule_interval = "x x x x xxx"
    v_email = ["dataeng@mecca.com.au", "{{user_email}}"]
    v_execution_timeout = timedelta(minutes=120)
    v_retry_delay = timedelta(minutes=1)
    v_retries = 0
elif env == "nonprod":
    v_start_date = datetime({{year}}, {{month}}, {{day}}, 0, 0, tzinfo=p_timezone)
    v_schedule_interval = None
    v_email = "{{user_email}}"
    v_execution_timeout = timedelta(minutes=120)
    v_retry_delay = timedelta(minutes=1)
    v_retries = 0
else:
    raise ValueError("Check Airflow environment variable exists")

default_args = dict(
    owner=p_owner,
    email=v_email,
    email_on_failure=True,
    email_on_retry=True,
    retries=v_retries,
    retry_delay=v_retry_delay,
    sla=None,
    execution_timeout=v_execution_timeout,
    depends_on_past=False,
    task_concurrency=16,
    pool=p_pool,
    provide_context=True,
)


def incremental_load_into_{{staging_table}}(**kwargs):
    """Incremental load and insert -> {{staging_schema}}.{{staging_table}}."""

    glinda_hook = PostgresHook(glinda_conn_id)

    increment_from, increment_to = glinda_hook.get_first("""
        SELECT
            increment_from::timestamptz-overlap::interval
            , now()
        FROM
            adm.table_parameters
        WHERE
            table_name = '{{schema}}.{{table}}';
    """)

    sql_save_increment_to = f"""
        SELECT
            adm.table_parameters_update('{{main_schema}}.{{main_table}}', 'increment_from_2', '{increment_to}');
    """

    sql_truncate = """
        TRUNCATE TABLE {{staging_schema}}.{{staging_table}}
    """

    sql_insert_staging = f"""
        INSERT INTO {{staging_schema}}.{{staging_table}} AS staging (
            {{staging_table_columns}}
        )
        SELECT
            {{staging_table_aliased_columns}}
        FROM
            {{staging_table_sources}}
        WHERE
            fs.record_updated >= '{increment_from}'::timestamptz
            AND fs.record_updated < '{increment_to}'::timestamptz;
        """

    glinda_hook.run(sql_save_increment_to, autocommit=True)
    glinda_hook.run(sql_truncate, autocommit=True)
    glinda_hook.run(sql_insert_staging, autocommit=True)


def upsert_into_{{table_name}}(**kwargs):
    """Upsert from staging -> {{schema}}.{{table}}."""

    glinda_hook = PostgresHook(glinda_conn_id)

    sql_upsert_{{schema}} = f"""
        INSERT INTO {{schema}}.{{table}} AS {{schema}} (
            {{columns}}
        )
        SELECT
            {{aliased_columns}}
        FROM
            {{staging_schema}}.{{staging_table}} {{staging_schema}}
        ON CONFLICT
            ({{primary_key_list}})
        DO UPDATE SET
            {{update_set}}
        WHERE (
            {{update_where}}
        );
    """

    sql_update_increment_from = """
        UPDATE
            adm.table_parameters
        SET
            increment_from = increment_from_2
        WHERE
            table_name = '{{schema}}.{{table}}';
    """

    glinda_hook.run(sql_upsert_{{schema}}, autocommit=True)
    glinda_hook.run(sql_update_increment_from, autocommit=True)

{{refresh_materialized_view}}

with DAG(
    dag_id=p_dag_id,
    description=p_description,
    schedule_interval=v_schedule_interval,
    start_date=v_start_date,
    default_args=default_args,
    concurrency=16,
    max_active_runs=1,
    catchup=False,
) as dag:
    t_incremental_load_into_{{staging_table}} = PythonOperator(
        task_id='incremental_load_into_{{staging_table}}',
        python_callable=incremental_load_into_{{staging_table}},
    )

    t_upsert_into_{{table_name}} = PythonOperator(
        task_id='upsert_into_{{table_name}}',
        python_callable=upsert_into_{{table_name}},
    )

t_incremental_load_into_{{staging_table}} >> t_upsert_into_{{table_name}}
